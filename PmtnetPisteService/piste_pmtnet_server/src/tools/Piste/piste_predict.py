from __future__ import division
import argparse
import numpy as np
import os
import pandas as pd
import random
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as Data
import torch.nn.functional as F
import uuid
import warnings

from collections import Counter
from minio import Minio
from minio.error import S3Error
from pathlib import Path
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score, auc
from sklearn.metrics import precision_recall_curve

#1

current_file = Path(__file__).resolve()
project_root = current_file.parents[3]
sys.path.append(str(project_root))
from config import CONFIG_YAML
from src.utils.log import logger

sys.path.append('/opt/softwares/PISTE')
from Model.PISTE import Transformer
# MinIO 配置:
MINIO_CONFIG = CONFIG_YAML["MINIO"]
MINIO_ENDPOINT = MINIO_CONFIG["endpoint"]
MINIO_ACCESS_KEY = os.getenv("ACCESS_KEY")
MINIO_SECRET_KEY = os.getenv("SECRET_KEY")
MINIO_BUCKET = MINIO_CONFIG["piste_bucket"]
MINIO_SECURE = MINIO_CONFIG.get("secure", False)

# 初始化 MinIO 客户端
minio_client = Minio(
    MINIO_ENDPOINT,
    access_key=MINIO_ACCESS_KEY,
    secret_key=MINIO_SECRET_KEY,
    secure=MINIO_SECURE
)


parser = argparse.ArgumentParser(usage = 'TCR-ANTIGEN-HLA binding prediction')
parser.add_argument('--input', type = str, help = 'the path to the input data file (*.csv).')
parser.add_argument("--model_name", default='random', type=str, choices=['random', 'unipep', 'reftcr'],
                        help="Choose different trained model by using datasets generated by different negative datasampling.")
parser.add_argument('--threshold', type = float, default = 0.5, help = 'the threshold to define predicted binder, float from 0 - 1, the recommended value is 0.5')
parser.add_argument('--antigen_type', type = str, default = 'MT', help = 'the antigen type, choice["MT","WT"]')
parser.add_argument('--output', type = str, help = 'The directory where the output results are stored(*.csv).')
args = parser.parse_args()

errLogPath = args.output + '/error.log'
if not os.path.exists(args.output): os.makedirs(args.output)
# if args.threshold <= 0 or args.threshold >= 1:
#     log = Logger(errLogPath)
#     log.logger.critical('The threshold invalid, please check whether it ranges from 0-1.')
#     sys.exit(0)
#
# errLogPath = args.output + '/error.log'
# if not args.input:
#     log = Logger(errLogPath)
#     log.logger.critical('The input file is empty.')
#     sys.exit(0)
# if not args.output:
#     log = Logger(errLogPath)
#     log.logger.critical('Please fill the output file directory.')
#     sys.exit(0)
# if not os.path.exists(args.output): os.makedirs(args.output)


random.seed(1234)
warnings.filterwarnings("ignore")

pep_max_len = 11
hla_max_len = 34
tcr_max_len = 30
tgt_len = pep_max_len + hla_max_len + tcr_max_len

batch_size = 1024
# epochs = 60
threshold = 0.5
d_model = 64
dim = 64
d_ff = 512
e_layers = 3
n_heads = 9
sigma = 1
d_layers = 1
interact_layers = 1
window_size = '3'

use_cuda = torch.cuda.is_available()
device = torch.device("cuda:0" if use_cuda else "cpu")
vocab = {'C': 1, 'W': 2, 'V': 3, 'A': 4, 'H': 5, 'T': 6, 'E': 7, 'K': 8, 'N': 9, 'P': 10, 'I': 11, 'L': 12, 'S': 13, 'D': 14, 'G': 15, 'Q': 16, 'R': 17, 'Y': 18, 'F': 19, 'M': 20, '-': 0}
vocab_size = len(vocab)
f_mean = lambda l: sum(l) / len(l)

def read_predict_data(predict_data, antigen_type, batch_size):
    
    column_names = predict_data.columns
    if "HLA_sequence" not in column_names:
    
        hla_sequence = pd.read_csv(r'/opt/softwares/PISTE/common_hla_sequence.csv')# 读取HLA序列文件 修改的地方
        predict_data = pd.merge(predict_data, hla_sequence, on = 'HLA_type')
        
    pep_inputs, hla_inputs, tcr_inputs = make_data(predict_data, antigen_type)
    loader = Data.DataLoader(MyDataSet(pep_inputs, hla_inputs, tcr_inputs), batch_size, shuffle=False, num_workers=0)
    return predict_data, pep_inputs, hla_inputs, tcr_inputs, loader

class MyDataSet(Data.Dataset):
    def __init__(self, pep_inputs, hla_inputs, tcr_inputs):
        super(MyDataSet, self).__init__()
        self.pep_inputs = pep_inputs
        self.hla_inputs = hla_inputs
        self.tcr_inputs = tcr_inputs

    def __len__(self):
        return self.pep_inputs.shape[0]

    def __getitem__(self, idx):
        return self.pep_inputs[idx], self.hla_inputs[idx], self.tcr_inputs[idx]


class WeightedFocalLoss(nn.Module):
    "Non weighted version of Focal Loss"

    def __init__(self, alpha=.25, gamma=2):
        super(WeightedFocalLoss, self).__init__()
        self.alpha = torch.tensor([alpha, 1 - alpha]).to(device)
        self.gamma = gamma

    def forward(self, inputs, targets):
        # BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')
        targets = targets.type(torch.long)
        at = self.alpha.gather(0, targets.data.view(-1))
        pt = torch.exp(-BCE_loss)
        F_loss = at * (1 - pt) ** self.gamma * BCE_loss
        return F_loss.mean()

def make_data(data, type):
    pep_inputs, hla_inputs, tcr_inputs = [], [], []
    num = 0
    
    if type == 'WT':
        for pep, hla, tcr in zip(data.WT_pep, data.HLA_sequence, data.CDR3):
            pep, hla, tcr = pep.ljust(pep_max_len, '-'), hla.ljust(hla_max_len, '-'), tcr.ljust(tcr_max_len, '-')

            pep_input = [[vocab[n] for n in pep]]
            hla_input = [[vocab[n] for n in hla]]
            tcr_input = [[vocab[n] for n in tcr]]
            pep_inputs.extend(pep_input)
            hla_inputs.extend(hla_input)
            tcr_inputs.extend(tcr_input)
            
            num = num + 1
            
    else:
        for pep, hla, tcr in zip(data.MT_pep, data.HLA_sequence, data.CDR3):
            pep, hla, tcr = pep.ljust(pep_max_len, '-'), hla.ljust(hla_max_len, '-'), tcr.ljust(tcr_max_len, '-')

            pep_input = [[vocab[n] for n in pep]]
            hla_input = [[vocab[n] for n in hla]]
            tcr_input = [[vocab[n] for n in tcr]]
            pep_inputs.extend(pep_input)
            hla_inputs.extend(hla_input)
            tcr_inputs.extend(tcr_input)
            
            num = num + 1
    

    return torch.LongTensor(pep_inputs), torch.LongTensor(hla_inputs), torch.LongTensor(tcr_inputs)


def transfer(y_prob, threshold=0.5):
    return np.array([[0, 1][x > threshold] for x in y_prob])


def eval_step(model, val_loader, threshold = 0.5, use_cuda = False):
    device = torch.device("cuda" if use_cuda else "cpu")
    
    model.eval()
    torch.manual_seed(19961231)
    torch.cuda.manual_seed(19961231)
    with torch.no_grad():
        y_prob_val_list, dec_attns_val_list = [], []
        for val_pep_inputs, val_hla_inputs, val_tcr_inputs in val_loader:
            val_pep_inputs, val_hla_inputs, val_tcr_inputs = val_pep_inputs.to(device), val_hla_inputs.to(device), val_tcr_inputs.to(device)
            
            val_outputs, _, val_dec_self_attns = model(val_pep_inputs, val_hla_inputs, val_tcr_inputs)
            
            y_prob_val = nn.Softmax(dim=1)(val_outputs)[:, 1].cpu().detach().numpy()
            y_prob_val_list.extend(y_prob_val)
            
            dec_attns_val_list.extend(val_dec_self_attns[0][:, :, 15:, :15]) 
                    
        y_pred_val_list = transfer(y_prob_val_list, threshold)
    
    return y_pred_val_list, y_prob_val_list, dec_attns_val_list

def upload_to_minio(minio_client, local_file_path, minio_bucket, minio_object_name):
    """
    使用 MinIO 客户端将文件上传到 MinIO 服务器，并返回 MinIO 路径。

    :param minio_client: MinIO 客户端实例
    :param local_file_path: 本地文件路径
    :param minio_bucket: MinIO 存储桶名称
    :param minio_object_name: MinIO 对象名称
    :return: 上传文件的 MinIO 路径，格式为 minio://bucket/object
    """
    try:
        # 上传文件
        minio_client.fput_object(
            minio_bucket,
            minio_object_name,
            local_file_path
        )
        # 构造并返回 MinIO 路径
        minio_path = f"minio://{minio_bucket}/{minio_object_name}"
        logger.info(f"MinIO path: {minio_path}")
        print(f"MinIO path: {minio_path}")
        return minio_path
    except S3Error as e:
        logger.error(f"MinIO S3 Error: {e}")
        raise
    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}")
        raise

antigen_type = args.antigen_type
predict_data = pd.read_csv(args.input)


predict_data, predict_pep_inputs, predict_hla_inputs, predict_tcr_inputs,predict_loader = read_predict_data(predict_data, antigen_type, batch_size)


model = Transformer(device=device,
                    vocab_size=vocab_size,
                    d_model=d_model,
                    e_layers=e_layers,
                    d=dim,
                    n_heads=n_heads,
                    sigma=sigma,
                    window_threshold=window_size,
                    d_ff=d_ff,
                    interact_layers=interact_layers,
                    tgt_len=tgt_len,
                    hla_max_len=hla_max_len,
                    d_layers=d_layers).to(device)
criterion = WeightedFocalLoss(alpha=.75)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# 加载模型
model_dir = r'/opt/softwares/PISTE/checkpoints/'#checkpoints#2
dir_saver = os.path.join(model_dir, args.model_name)
path_saver = os.path.join(dir_saver, "exp0.pkl")
#print(path_saver)

model.load_state_dict(torch.load(path_saver, map_location=device))######
model_eval = model.eval()

y_pred, y_prob, attns = eval_step(model_eval, predict_loader, 0.5, use_cuda)

predict_data['predicted_label'], predict_data['predicted_score'] = y_pred, y_prob
object_name = f"{uuid.uuid4()}_PISTE_results.csv"
output_file_to_local = f"{args.output}/{object_name}"
predict_data.to_csv(output_file_to_local, index=0)

#upload to minio
if not minio_client.bucket_exists(MINIO_BUCKET):
    minio_client.make_bucket(MINIO_BUCKET)
minio_path = upload_to_minio(minio_client, output_file_to_local, MINIO_BUCKET, object_name)
if minio_path.startswith("minio://") :
    try:
        if not isinstance(output_file_to_local, Path):
                output_file_to_local = Path(output_file_to_local)
        if output_file_to_local.exists():
            output_file_to_local.unlink()
            logger.info(f"Deleted local file: {output_file_to_local}")
            # print(f"Deleted local file: {output_file_to_local}")
        else:
            logger.warning(f"Local file does not exist: {output_file_to_local}")
            # print(f"Local file does not exist: {output_file_to_local}")
    except Exception as e:
        logger.error(f"Error deleting local file: {e}")
print('Prediction is done.')










